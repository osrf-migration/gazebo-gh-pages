{"priority": "major", "kind": "proposal", "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "links": {"attachments": {"href": "data/repositories/osrf/gazebo/issues/309/attachments_page=1.json"}, "self": {"href": "data/repositories/osrf/gazebo/issues/309.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/gazebo/issues/309/watch"}, "comments": {"href": "data/repositories/osrf/gazebo/issues/309/comments_page=1.json"}, "html": {"href": "#!/osrf/gazebo/issues/309/use-of-visual-model-instead-of-collision"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/gazebo/issues/309/vote"}}, "reporter": {"display_name": "Thomas Koletschka", "uuid": "{b33092ac-6376-48d1-9803-d9e6de445d1a}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bb33092ac-6376-48d1-9803-d9e6de445d1a%7D"}, "html": {"href": "https://bitbucket.org/%7Bb33092ac-6376-48d1-9803-d9e6de445d1a%7D/"}, "avatar": {"href": "https://bitbucket.org/account/thomasko/avatar/"}}, "nickname": "thomasko", "type": "user", "account_id": null}, "title": "use of visual model instead of collision model for lidar sensors", "component": null, "votes": 0, "watches": 2, "content": {"raw": "Currently lidar sensors (or any ray sensor in general) use the physics engine for ray tracing. This results in proximity sensors perceiving a different world than optical sensors which is fine as long as those two are the same but once you start simplifying and abstracting models and worlds this becomes a very big problem. \r\n\r\nFor example when you want to add grass or other vegetation as a perception obstructing element you could use just a visual model and no collision model, which would then allow proximity sensors to \"cheat\" and see through to the ground. Another example is the use of very simplified objects when the robot only has to interact with some part of it or not at all but still needs to be able to recognize the object.\r\n\r\nTransparency of objects (alpha channel in the object's texture) should be considered as well - i.e. the beam should go through transparent/missing parts of the model in order to perceive the same shape as camera sensors.", "markup": "markdown", "html": "<p>Currently lidar sensors (or any ray sensor in general) use the physics engine for ray tracing. This results in proximity sensors perceiving a different world than optical sensors which is fine as long as those two are the same but once you start simplifying and abstracting models and worlds this becomes a very big problem. </p>\n<p>For example when you want to add grass or other vegetation as a perception obstructing element you could use just a visual model and no collision model, which would then allow proximity sensors to \"cheat\" and see through to the ground. Another example is the use of very simplified objects when the robot only has to interact with some part of it or not at all but still needs to be able to recognize the object.</p>\n<p>Transparency of objects (alpha channel in the object's texture) should be considered as well - i.e. the beam should go through transparent/missing parts of the model in order to perceive the same shape as camera sensors.</p>", "type": "rendered"}, "assignee": null, "state": "closed", "version": {"name": "all", "links": {"self": {"href": "data/repositories/osrf/gazebo/versions/225575.json"}}}, "edited_on": null, "created_on": "2012-12-16T04:59:57.275056+00:00", "milestone": null, "updated_on": "2016-09-14T19:44:39.391663+00:00", "type": "issue", "id": 309}