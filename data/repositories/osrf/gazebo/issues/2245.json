{"priority": "major", "kind": "proposal", "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "links": {"attachments": {"href": "data/repositories/osrf/gazebo/issues/2245/attachments_page=1.json"}, "self": {"href": "data/repositories/osrf/gazebo/issues/2245.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/gazebo/issues/2245/watch"}, "comments": {"href": "data/repositories/osrf/gazebo/issues/2245/comments_page=1.json"}, "html": {"href": "#!/osrf/gazebo/issues/2245/px4-rotors-sitl-support-requires-sensor"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/gazebo/issues/2245/vote"}}, "reporter": {"display_name": "James Goppert", "uuid": "{b618f3e0-62ee-44c4-bc34-e54b9f05b140}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bb618f3e0-62ee-44c4-bc34-e54b9f05b140%7D"}, "html": {"href": "https://bitbucket.org/%7Bb618f3e0-62ee-44c4-bc34-e54b9f05b140%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:169ff364-18d4-4ba3-bf5a-4fd0ab88c331/c1565592-9846-41bd-836a-8421cebab1f4/128"}}, "nickname": "jgoppert", "type": "user", "account_id": "557058:169ff364-18d4-4ba3-bf5a-4fd0ab88c331"}, "title": "PX4/rotors SITL support requires sensor level data", "component": null, "votes": 0, "watches": 3, "content": {"raw": "I am trying to integrate the PX4 and rotors gazebo plugins back into Gazebo upstream. I took a look at the ArduCopter plugin, but it seems this sends a limited set of data over a socket (position, velocity, accelerate, angular velocity), some of this is passed form the imu to get noise, but it doesn't meet the fidelity of our current gazebo plugins.\r\n\r\n### Sensor Level SITL Requirements\r\n\r\nFor PX4/rotors sitl we are using the native gazebo sensor models instead of reconstructing the data on the other side, we would need the following sensor packets:\r\n\r\n * Magnetometer\r\n * GPS lat/lon (to tie gazebo global position to SITL global position)\r\n * laser distance sensor\r\n * barometric altimeter sensor\r\n * collision avoidance sensor\r\n * optical flow sensor\r\n\r\n### Current Transport Approach (gazebo plugin) <-> mavlink <-> (px4 sitl)\r\n\r\nWe are currently using MAVLink as the transport protocol over a socket instead of just a custom struct. The advantage is that we can use the predefined sensor packet types in MAVLink. The disadvantage is adding a dependency for MAVLink to gazebo.\r\n\r\n### Alternate Approach: Use Gazebo Transport API (gazebo) <-> (gazebo pub/sub) <-> (px4 sitl)\r\n\r\nAnother approach would be to use the gazebo transport api on the px4 SITL side and subscribe to the corresponding topics for each vehicle. I think the question is which approach (gazebo transpose or MAVLink) would be the most robust and require the least software maintenance.\r\n\r\n### Optical Flow Plugin\r\n\r\nThe optical flow plugin depends on opencv. I investigated and, due to the avplugin, this is already an implicit dependency of gazebo. Instead of just faking the optical flow data, we actually compute optical flow given the camera image. Granted this takes a little bit of cpu, but it makes testing how texture, rotation rates, and the flow algorithm impact the performance. I think there should probably be a model that does this, and then another model that just calculates the values given the true velocities and adds a noise model to try to represent some of these effects.", "markup": "markdown", "html": "<p>I am trying to integrate the PX4 and rotors gazebo plugins back into Gazebo upstream. I took a look at the ArduCopter plugin, but it seems this sends a limited set of data over a socket (position, velocity, accelerate, angular velocity), some of this is passed form the imu to get noise, but it doesn't meet the fidelity of our current gazebo plugins.</p>\n<h3 id=\"markdown-header-sensor-level-sitl-requirements\">Sensor Level SITL Requirements</h3>\n<p>For PX4/rotors sitl we are using the native gazebo sensor models instead of reconstructing the data on the other side, we would need the following sensor packets:</p>\n<ul>\n<li>Magnetometer</li>\n<li>GPS lat/lon (to tie gazebo global position to SITL global position)</li>\n<li>laser distance sensor</li>\n<li>barometric altimeter sensor</li>\n<li>collision avoidance sensor</li>\n<li>optical flow sensor</li>\n</ul>\n<h3 id=\"markdown-header-current-transport-approach-gazebo-plugin-mavlink-px4-sitl\">Current Transport Approach (gazebo plugin) &lt;-&gt; mavlink &lt;-&gt; (px4 sitl)</h3>\n<p>We are currently using MAVLink as the transport protocol over a socket instead of just a custom struct. The advantage is that we can use the predefined sensor packet types in MAVLink. The disadvantage is adding a dependency for MAVLink to gazebo.</p>\n<h3 id=\"markdown-header-alternate-approach-use-gazebo-transport-api-gazebo-gazebo-pubsub-px4-sitl\">Alternate Approach: Use Gazebo Transport API (gazebo) &lt;-&gt; (gazebo pub/sub) &lt;-&gt; (px4 sitl)</h3>\n<p>Another approach would be to use the gazebo transport api on the px4 SITL side and subscribe to the corresponding topics for each vehicle. I think the question is which approach (gazebo transpose or MAVLink) would be the most robust and require the least software maintenance.</p>\n<h3 id=\"markdown-header-optical-flow-plugin\">Optical Flow Plugin</h3>\n<p>The optical flow plugin depends on opencv. I investigated and, due to the avplugin, this is already an implicit dependency of gazebo. Instead of just faking the optical flow data, we actually compute optical flow given the camera image. Granted this takes a little bit of cpu, but it makes testing how texture, rotation rates, and the flow algorithm impact the performance. I think there should probably be a model that does this, and then another model that just calculates the values given the true velocities and adds a noise model to try to represent some of these effects.</p>", "type": "rendered"}, "assignee": {"display_name": "John Hsu", "uuid": "{0a186eae-abf0-4514-a951-23db5eccc286}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B0a186eae-abf0-4514-a951-23db5eccc286%7D"}, "html": {"href": "https://bitbucket.org/%7B0a186eae-abf0-4514-a951-23db5eccc286%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:f3968cd3-4910-4384-8349-482a6c7889ec/5445ce6e-6273-47f0-84eb-621c86ca11cb/128"}}, "nickname": "hsu", "type": "user", "account_id": "557058:f3968cd3-4910-4384-8349-482a6c7889ec"}, "state": "new", "version": null, "edited_on": null, "created_on": "2017-04-01T22:40:37.548063+00:00", "milestone": null, "updated_on": "2017-04-03T15:27:28.233014+00:00", "type": "issue", "id": 2245}