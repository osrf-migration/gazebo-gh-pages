{"pagelen": 50, "values": [{"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/40329486.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-40329486"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "I don't understand the check `!this->pubIds.empty()`. It looks like `this->publication->Publish(...)` calls `CallbackHelper::HandleData(...)` which is an instance of `SubscriptionTransport` which calls `Connection::EnqueueMsg` which saves the bytes to write in a queue on the connection. Even if there are outstanding pubIds, aren't new messages still enqueued on each `Connection` in order they are published?", "markup": "markdown", "html": "<p>I don't understand the check <code>!this-&gt;pubIds.empty()</code>. It looks like <code>this-&gt;publication-&gt;Publish(...)</code> calls <code>CallbackHelper::HandleData(...)</code> which is an instance of <code>SubscriptionTransport</code> which calls <code>Connection::EnqueueMsg</code> which saves the bytes to write in a queue on the connection. Even if there are outstanding pubIds, aren't new messages still enqueued on each <code>Connection</code> in order they are published?</p>", "type": "rendered"}, "created_on": "2017-07-07T16:24:29.836380+00:00", "user": {"display_name": "Shane Loretz", "uuid": "{656e3311-aad9-45a1-aaf7-b0ee0e84b287}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B656e3311-aad9-45a1-aaf7-b0ee0e84b287%7D"}, "html": {"href": "https://bitbucket.org/%7B656e3311-aad9-45a1-aaf7-b0ee0e84b287%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:684383ab-ac95-4859-a350-4a6f41a94a22/c7a1ebf5-cade-4115-9f26-9d3facb776db/128"}}, "nickname": "Shane Loretz", "type": "user", "account_id": "557058:684383ab-ac95-4859-a350-4a6f41a94a22"}, "inline": {}, "updated_on": "2017-07-07T16:24:29.839524+00:00", "type": "pullrequest_comment", "id": 40329486}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36531900.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36531900"}}, "parent": {"id": 36527251, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36527251.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36527251"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "[It's failing](https://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/1080/consoleFull) (see test 461 in output log), though it's not listed in the [test failures overview](https://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/1080/#showFailuresLink), any idea why? In [this build](https://build.osrfoundation.org/job/gazebo-ci-pr_any-xenial-amd64-gpu-nvidia/166/) it is listed as failing...", "markup": "markdown", "html": "<p><a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/1080/consoleFull\" rel=\"nofollow\">It's failing</a> (see test 461 in output log), though it's not listed in the <a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/1080/#showFailuresLink\" rel=\"nofollow\">test failures overview</a>, any idea why? In <a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any-xenial-amd64-gpu-nvidia/166/\" rel=\"nofollow\">this build</a> it is listed as failing...</p>", "type": "rendered"}, "created_on": "2017-05-10T03:33:01.713467+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-10T03:33:01.783561+00:00", "type": "pullrequest_comment", "id": 36531900}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36527251.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36527251"}}, "parent": {"id": 36525181, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36525181.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36525181"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Added the same test file to the branch [transport_sendmessage_blocking_issue_test](https://bitbucket.org/JenniferBuehler/gazebo/branch/transport_sendmessage_blocking_issue_test#diff) and [queued a build](https://build.osrfoundation.org/job/gazebo-ci-pr_any/549/). The test should be failing.\n\nQuestion: Should we get rid of the macro ``TEMP_NEW_APPROACH_TEST `` now that we have the other branch with the failing test? Advantage of keeping it would be that you can easily test locally, switching between bugfix applied/not applied. Without the macro, we have to switch between branches and change the test accordingly in the other branch (if we apply changes to the test at all).", "markup": "markdown", "html": "<p>Added the same test file to the branch <a data-is-external-link=\"true\" href=\"https://bitbucket.org/JenniferBuehler/gazebo/branch/transport_sendmessage_blocking_issue_test#diff\" rel=\"nofollow\">transport_sendmessage_blocking_issue_test</a> and <a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any/549/\" rel=\"nofollow\">queued a build</a>. The test should be failing.</p>\n<p>Question: Should we get rid of the macro <code>TEMP_NEW_APPROACH_TEST</code> now that we have the other branch with the failing test? Advantage of keeping it would be that you can easily test locally, switching between bugfix applied/not applied. Without the macro, we have to switch between branches and change the test accordingly in the other branch (if we apply changes to the test at all).</p>", "type": "rendered"}, "created_on": "2017-05-10T00:13:34.849540+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-10T00:13:34.915431+00:00", "type": "pullrequest_comment", "id": 36527251}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36525181.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36525181"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "I fixed up the test so it now can handle automated testing. Started a [new build](https://build.osrfoundation.org/job/gazebo-ci-pr_any/548/). The test will not be failing because the bug fix is already applied.\nTo disable the bug fix and go back to old code, it is still necessary to define the macro TEMP_NEW_APPROACH_TEST in Publisher.cc, then the test fails.\n\nTo demonstrate how the test normally fails, I will create a branch directly from default with only this test, so we can see that it fails under normal circumstances. Will post the link here as soon as I've got that.", "markup": "markdown", "html": "<p>I fixed up the test so it now can handle automated testing. Started a <a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any/548/\" rel=\"nofollow\">new build</a>. The test will not be failing because the bug fix is already applied.\nTo disable the bug fix and go back to old code, it is still necessary to define the macro TEMP_NEW_APPROACH_TEST in Publisher.cc, then the test fails.</p>\n<p>To demonstrate how the test normally fails, I will create a branch directly from default with only this test, so we can see that it fails under normal circumstances. Will post the link here as soon as I've got that.</p>", "type": "rendered"}, "created_on": "2017-05-09T23:01:57.694399+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-09T23:01:57.765738+00:00", "type": "pullrequest_comment", "id": 36525181}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"update": {"description": "I believe I stumbled over a bug in the transport system which sometimes stalls the message publishing of a `transport::Publisher`. This can be triggered when using `Publisher::Publish()` in blocking mode (parameter \\_block=true).\r\n\r\nThis is a multi-threading related issue which I think should be fixed in any case (also for non-blocking mode).\r\n\r\nIt's probably easiest to explain the issue on code snippets of the existing code.\r\n\r\nFor easy reference: `OnPublishComplete` is the function which should be called at the time just after a message has just been sent (which can happen from another thread). It decreases a counter for this message ID, and deletes the entry from the buffer if the counter reaches zero. This basically helps to keep track on the number of times this function was called for each message.   \r\nNote that the `this->pubIds` map is protected by `this->mutex`.\r\n\r\n```C++\r\nvoid Publisher::OnPublishComplete(uint32_t _id) \r\n{ \r\n boost::mutex::scoped_lock lock(this->mutex); \r\n \r\n std::map<uint32_t, int>::iterator iter = this->pubIds.find(_id); \r\n if (iter != this->pubIds.end() && (--iter->second) <= 0) \r\n this->pubIds.erase(iter); \r\n} \r\n```\r\n\r\nThen there is the function `SendMessage()` which is called from `Publisher::Publish()` when in blocking mode. The blocking is supposed to wait until the message is \"written out\" (*minor side issue:* this is a phrasing in the comments that could maybe be changed, because the actual \"writing out\" still happens asynchronously from `Publication`, so a call of `Publisher::Publish(..block=true)` will not actually guarantee the message has been written out!).\r\n\r\n```C++\r\nvoid Publisher::SendMessage()                                                   \r\n{                                                                               \r\n  std::list<MessagePtr> localBuffer;                                            \r\n  std::list<uint32_t> localIds;                                                 \r\n                                                                                \r\n  {                                                                             \r\n    boost::mutex::scoped_lock lock(this->mutex);                                \r\n    if (!this->pubIds.empty() || this->messages.empty())                        \r\n      return;                                                                   \r\n                                                                                \r\n    for (unsigned int i = 0; i < this->messages.size(); ++i)                    \r\n    {                                                                           \r\n      this->pubId = (this->pubId + 1) % 10000;                                  \r\n      this->pubIds[this->pubId] = 0;                                            \r\n      localIds.push_back(this->pubId);                                          \r\n    }                                                                           \r\n                                                                                \r\n    std::copy(this->messages.begin(), this->messages.end(),                     \r\n        std::back_inserter(localBuffer));                                       \r\n    this->messages.clear();                                                     \r\n  }\r\n  ...\r\n```\r\n\r\nThis first part basically has copied everything into local buffers (presumably to avoid blocking the mutex too long and to avoid deadlocks). This is all trivial, except one crucial thing to note here: If there are any message ID's still hanging around in `this->pubID`, this function won't do anything more - no \"writing out\" happens. This can be interpreted as *\"if there are any messages which have not finished publishing yet, don't proceed, and instead keep them in the `this->messages `buffer until the next time SendMessage() is called\"*. I guess this was done in order to ensure that messages are sent out in the right order (?).\r\n\r\nNext, the copies of `messages` and `pubIDs` are used to actually publish the messages using `transport::Publication::Publish()`, which will trigger a call to `OnPublishComplete()` **once for each subscriber callback** of `Publication`. A subscriber callback however is used for **remote subscribers** only (started in a separate terminal). It's probably possible to add subscriber callbacks in different ways too. But here it's only important that if we have local subscribers (as in all the gtests as far as I can see), we won't have a subscriber callback.   \r\n*Bottom line:* `Publication::Publish()` basically returns the number of times the `OnPublishComplete()` callback has been triggered per remote subscriber (Note that with remote subscribers, the actual calling of OnPublishComplete() happens asynchronously: the subscriber callback `SubscriptionTransport::HandleData()` only enqueues the message).\r\n\r\n```C++\r\n  ...\r\n  // Only send messages if there is something to send                           \r\n  if (!localBuffer.empty())                                                     \r\n  {                                                                             \r\n    std::list<uint32_t>::iterator pubIter = localIds.begin();                   \r\n                                                                                \r\n    // Send all the current messages                                            \r\n    for (std::list<MessagePtr>::iterator iter = localBuffer.begin();            \r\n        iter != localBuffer.end(); ++iter, ++pubIter)                           \r\n    {                                                                           \r\n      // Send the latest message.                                               \r\n      int result = this->publication->Publish(*iter,                            \r\n          boost::bind(&Publisher::OnPublishComplete, this, _1), *pubIter);\r\n      ...\r\n```\r\n\r\nSo I believe the intention is to expect a certain amount of calls to `OnPublishComplete()`. To achieve this, the `this->pubIds` is set with the message ID as key, and as a value the expected number of calls:\r\n\r\n```C++\r\n      ...\r\n      if (result > 0)                                                           \r\n        this->pubIds[*pubIter] = result;                                        \r\n      else                                                                      \r\n        this->pubIds.erase(*pubIter);                                           \r\n    }\r\n    ...\r\n```\r\n\r\nPlease correct me if I have misunderstood this intention.\r\n\r\nThere is a few issues with the above bit of code:\r\n\r\n*  `pubIds` is not mutex protected as it should be.\r\n* If we have only local subscribers, the counter should still be set to 1, because `OnPublishComplete` will still be called once.\r\n* It is possible (randomly due to threads) that by the time `this->pubIds` is set to the value of `result`, `OnPublishComplete()` has already been called, and the entry in `pubIds` has already been erased. This has happened to me, which brought me onto this issue in the first place. In this case, if we have remote subscribers, we will just re-add it here, and **it will never be removed again**! Which will in turn cause all future calls to `SendMessage()` to not do anything, as per condition `!this->pubIds.empty()` in the beginning of the function. So we have a \"publishing dead-end\".\r\n\r\nThe tricky bit is that this only happens when using remote subscribers, which may be a reason why it has gone unnoticed in the automated tests so far? Because if there are only local subscribers, `result` is 0, and the buffer will just always be cleared (it will not really be used actually). So the issue won't arise at all.\r\n\r\nThis PR proposes a way to fix this up, though it's not ready to be merged as there could be better (but more intrusive) solutions to this. I've put it up for discussion. I added some more comments to the code to explain what's going on. \r\n\r\nI've also added a test file which still requires to start a remote publisher manually (by `gz topic -e <topic>`), which is something that should be simulated instead.  \r\nYou can trigger the failure case by commenting `TEMP_NEW_APPROACH_TEST` in Publisher.cc, which will then use the old code. You may have to try a few times because of the randomness, sometimes it can still succeed with the old code.\r\n\r\nOne last thing: It would be important to let the user know that if they are only using \"blocking\" calls to `Publisher::Publish()`, it is *not* guaranteed that the message has actually already been sent. It may still sit in the queue, and it may actually be necessary to call `Publisher::SendMessage()` again to make sure the queue has been emptied. See also comments in the code.", "title": "Fixing issue in which publishing of messages gets stuck.", "destination": {"commit": {"hash": "6f343883a0b2", "type": "commit", "links": {"self": {"href": "data/repositories/osrf/gazebo/commit/6f343883a0b2.json"}, "html": {"href": "#!/osrf/gazebo/commits/6f343883a0b2"}}}, "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "031e74001361", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo/commit/031e74001361"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo/commits/031e74001361"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{11522dd5-648b-4e9e-b908-d3e1170ba728}ts=c_plus_plus"}}, "type": "repository", "name": "gazebo", "full_name": "JenniferBuehler/gazebo", "uuid": "{11522dd5-648b-4e9e-b908-d3e1170ba728}"}, "branch": {"name": "transport_sendmessage_blocking_issue"}}, "state": "OPEN", "author": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "date": "2017-05-09T22:56:18.189581+00:00"}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36372804.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36372804"}}, "parent": {"id": 36372724, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36372724.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36372724"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Ahrmm... brings me to the next question: Any hint on how to simulate a true remote connection? If I just start a subscriber locally in the test source, it won't be truly remote, so it won't work.", "markup": "markdown", "html": "<p>Ahrmm... brings me to the next question: Any hint on how to simulate a true remote connection? If I just start a subscriber locally in the test source, it won't be truly remote, so it won't work.</p>", "type": "rendered"}, "created_on": "2017-05-08T06:35:49.299368+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-08T06:35:49.342936+00:00", "type": "pullrequest_comment", "id": 36372804}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36372724.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36372724"}}, "parent": {"id": 33684600, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33684600.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33684600"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "How silly of me... the timeout of course happens because this test is not designed to run in automated mode yet, it still requires you to [start a test subscriber in another terminal](#!/osrf/gazebo/pull-requests/2657/fixing-issue-in-which-publishing-of/diff#Ltest/integration/transport_msg_count.ccT90). I'll add the automated version for this.", "markup": "markdown", "html": "<p>How silly of me... the timeout of course happens because this test is not designed to run in automated mode yet, it still requires you to <a data-is-external-link=\"true\" href=\"#!/osrf/gazebo/pull-requests/2657/fixing-issue-in-which-publishing-of/diff#Ltest/integration/transport_msg_count.ccT90\" rel=\"nofollow\">start a test subscriber in another terminal</a>. I'll add the automated version for this.</p>", "type": "rendered"}, "created_on": "2017-05-08T06:34:13.235210+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-08T06:34:13.296086+00:00", "type": "pullrequest_comment", "id": 36372724}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/36370001.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-36370001"}}, "parent": {"id": 33684600, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33684600.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33684600"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Started [another build](https://build.osrfoundation.org/job/gazebo-ci-pr_any/547/) because the link doesn't work any more", "markup": "markdown", "html": "<p>Started <a data-is-external-link=\"true\" href=\"https://build.osrfoundation.org/job/gazebo-ci-pr_any/547/\" rel=\"nofollow\">another build</a> because the link doesn't work any more</p>", "type": "rendered"}, "created_on": "2017-05-08T05:03:03.359046+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-05-08T05:03:03.422682+00:00", "type": "pullrequest_comment", "id": 36370001}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"update": {"description": "I believe I stumbled over a bug in the transport system which sometimes stalls the message publishing of a `transport::Publisher`. This can be triggered when using `Publisher::Publish()` in blocking mode (parameter \\_block=true).\r\n\r\nThis is a multi-threading related issue which I think should be fixed in any case (also for non-blocking mode).\r\n\r\nIt's probably easiest to explain the issue on code snippets of the existing code.\r\n\r\nFor easy reference: `OnPublishComplete` is the function which should be called at the time just after a message has just been sent (which can happen from another thread). It decreases a counter for this message ID, and deletes the entry from the buffer if the counter reaches zero. This basically helps to keep track on the number of times this function was called for each message.   \r\nNote that the `this->pubIds` map is protected by `this->mutex`.\r\n\r\n```C++\r\nvoid Publisher::OnPublishComplete(uint32_t _id) \r\n{ \r\n boost::mutex::scoped_lock lock(this->mutex); \r\n \r\n std::map<uint32_t, int>::iterator iter = this->pubIds.find(_id); \r\n if (iter != this->pubIds.end() && (--iter->second) <= 0) \r\n this->pubIds.erase(iter); \r\n} \r\n```\r\n\r\nThen there is the function `SendMessage()` which is called from `Publisher::Publish()` when in blocking mode. The blocking is supposed to wait until the message is \"written out\" (*minor side issue:* this is a phrasing in the comments that could maybe be changed, because the actual \"writing out\" still happens asynchronously from `Publication`, so a call of `Publisher::Publish(..block=true)` will not actually guarantee the message has been written out!).\r\n\r\n```C++\r\nvoid Publisher::SendMessage()                                                   \r\n{                                                                               \r\n  std::list<MessagePtr> localBuffer;                                            \r\n  std::list<uint32_t> localIds;                                                 \r\n                                                                                \r\n  {                                                                             \r\n    boost::mutex::scoped_lock lock(this->mutex);                                \r\n    if (!this->pubIds.empty() || this->messages.empty())                        \r\n      return;                                                                   \r\n                                                                                \r\n    for (unsigned int i = 0; i < this->messages.size(); ++i)                    \r\n    {                                                                           \r\n      this->pubId = (this->pubId + 1) % 10000;                                  \r\n      this->pubIds[this->pubId] = 0;                                            \r\n      localIds.push_back(this->pubId);                                          \r\n    }                                                                           \r\n                                                                                \r\n    std::copy(this->messages.begin(), this->messages.end(),                     \r\n        std::back_inserter(localBuffer));                                       \r\n    this->messages.clear();                                                     \r\n  }\r\n  ...\r\n```\r\n\r\nThis first part basically has copied everything into local buffers (presumably to avoid blocking the mutex too long and to avoid deadlocks). This is all trivial, except one crucial thing to note here: If there are any message ID's still hanging around in `this->pubID`, this function won't do anything more - no \"writing out\" happens. This can be interpreted as *\"if there are any messages which have not finished publishing yet, don't proceed, and instead keep them in the `this->messages `buffer until the next time SendMessage() is called\"*. I guess this was done in order to ensure that messages are sent out in the right order (?).\r\n\r\nNext, the copies of `messages` and `pubIDs` are used to actually publish the messages using `transport::Publication::Publish()`, which will trigger a call to `OnPublishComplete()` **once for each subscriber callback** of `Publication`. A subscriber callback however is used for **remote subscribers** only (started in a separate terminal). It's probably possible to add subscriber callbacks in different ways too. But here it's only important that if we have local subscribers (as in all the gtests as far as I can see), we won't have a subscriber callback.   \r\n*Bottom line:* `Publication::Publish()` basically returns the number of times the `OnPublishComplete()` callback has been triggered per remote subscriber (Note that with remote subscribers, the actual calling of OnPublishComplete() happens asynchronously: the subscriber callback `SubscriptionTransport::HandleData()` only enqueues the message).\r\n\r\n```C++\r\n  ...\r\n  // Only send messages if there is something to send                           \r\n  if (!localBuffer.empty())                                                     \r\n  {                                                                             \r\n    std::list<uint32_t>::iterator pubIter = localIds.begin();                   \r\n                                                                                \r\n    // Send all the current messages                                            \r\n    for (std::list<MessagePtr>::iterator iter = localBuffer.begin();            \r\n        iter != localBuffer.end(); ++iter, ++pubIter)                           \r\n    {                                                                           \r\n      // Send the latest message.                                               \r\n      int result = this->publication->Publish(*iter,                            \r\n          boost::bind(&Publisher::OnPublishComplete, this, _1), *pubIter);\r\n      ...\r\n```\r\n\r\nSo I believe the intention is to expect a certain amount of calls to `OnPublishComplete()`. To achieve this, the `this->pubIds` is set with the message ID as key, and as a value the expected number of calls:\r\n\r\n```C++\r\n      ...\r\n      if (result > 0)                                                           \r\n        this->pubIds[*pubIter] = result;                                        \r\n      else                                                                      \r\n        this->pubIds.erase(*pubIter);                                           \r\n    }\r\n    ...\r\n```\r\n\r\nPlease correct me if I have misunderstood this intention.\r\n\r\nThere is a few issues with the above bit of code:\r\n\r\n*  `pubIds` is not mutex protected as it should be.\r\n* If we have only local subscribers, the counter should still be set to 1, because `OnPublishComplete` will still be called once.\r\n* It is possible (randomly due to threads) that by the time `this->pubIds` is set to the value of `result`, `OnPublishComplete()` has already been called, and the entry in `pubIds` has already been erased. This has happened to me, which brought me onto this issue in the first place. In this case, if we have remote subscribers, we will just re-add it here, and **it will never be removed again**! Which will in turn cause all future calls to `SendMessage()` to not do anything, as per condition `!this->pubIds.empty()` in the beginning of the function. So we have a \"publishing dead-end\".\r\n\r\nThe tricky bit is that this only happens when using remote subscribers, which may be a reason why it has gone unnoticed in the automated tests so far? Because if there are only local subscribers, `result` is 0, and the buffer will just always be cleared (it will not really be used actually). So the issue won't arise at all.\r\n\r\nThis PR proposes a way to fix this up, though it's not ready to be merged as there could be better (but more intrusive) solutions to this. I've put it up for discussion. I added some more comments to the code to explain what's going on. \r\n\r\nI've also added a test file which still requires to start a remote publisher manually (by `gz topic -e <topic>`), which is something that should be simulated instead.  \r\nYou can trigger the failure case by commenting `TEMP_NEW_APPROACH_TEST` in Publisher.cc, which will then use the old code. You may have to try a few times because of the randomness, sometimes it can still succeed with the old code.\r\n\r\nOne last thing: It would be important to let the user know that if they are only using \"blocking\" calls to `Publisher::Publish()`, it is *not* guaranteed that the message has actually already been sent. It may still sit in the queue, and it may actually be necessary to call `Publisher::SendMessage()` again to make sure the queue has been emptied. See also comments in the code.", "title": "Fixing issue in which publishing of messages gets stuck.", "destination": {"commit": {"hash": "7d7c37d66d00", "type": "commit", "links": {"self": {"href": "data/repositories/osrf/gazebo/commit/7d7c37d66d00.json"}, "html": {"href": "#!/osrf/gazebo/commits/7d7c37d66d00"}}}, "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "3ee9a5f675e0", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo/commit/3ee9a5f675e0"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo/commits/3ee9a5f675e0"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{11522dd5-648b-4e9e-b908-d3e1170ba728}ts=c_plus_plus"}}, "type": "repository", "name": "gazebo", "full_name": "JenniferBuehler/gazebo", "uuid": "{11522dd5-648b-4e9e-b908-d3e1170ba728}"}, "branch": {"name": "transport_sendmessage_blocking_issue"}}, "state": "OPEN", "author": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "date": "2017-05-08T04:32:31.822669+00:00"}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"update": {"description": "I believe I stumbled over a bug in the transport system which sometimes stalls the message publishing of a `transport::Publisher`. This can be triggered when using `Publisher::Publish()` in blocking mode (parameter \\_block=true).\r\n\r\nThis is a multi-threading related issue which I think should be fixed in any case (also for non-blocking mode).\r\n\r\nIt's probably easiest to explain the issue on code snippets of the existing code.\r\n\r\nFor easy reference: `OnPublishComplete` is the function which should be called at the time just after a message has just been sent (which can happen from another thread). It decreases a counter for this message ID, and deletes the entry from the buffer if the counter reaches zero. This basically helps to keep track on the number of times this function was called for each message.   \r\nNote that the `this->pubIds` map is protected by `this->mutex`.\r\n\r\n```C++\r\nvoid Publisher::OnPublishComplete(uint32_t _id) \r\n{ \r\n boost::mutex::scoped_lock lock(this->mutex); \r\n \r\n std::map<uint32_t, int>::iterator iter = this->pubIds.find(_id); \r\n if (iter != this->pubIds.end() && (--iter->second) <= 0) \r\n this->pubIds.erase(iter); \r\n} \r\n```\r\n\r\nThen there is the function `SendMessage()` which is called from `Publisher::Publish()` when in blocking mode. The blocking is supposed to wait until the message is \"written out\" (*minor side issue:* this is a phrasing in the comments that could maybe be changed, because the actual \"writing out\" still happens asynchronously from `Publication`, so a call of `Publisher::Publish(..block=true)` will not actually guarantee the message has been written out!).\r\n\r\n```C++\r\nvoid Publisher::SendMessage()                                                   \r\n{                                                                               \r\n  std::list<MessagePtr> localBuffer;                                            \r\n  std::list<uint32_t> localIds;                                                 \r\n                                                                                \r\n  {                                                                             \r\n    boost::mutex::scoped_lock lock(this->mutex);                                \r\n    if (!this->pubIds.empty() || this->messages.empty())                        \r\n      return;                                                                   \r\n                                                                                \r\n    for (unsigned int i = 0; i < this->messages.size(); ++i)                    \r\n    {                                                                           \r\n      this->pubId = (this->pubId + 1) % 10000;                                  \r\n      this->pubIds[this->pubId] = 0;                                            \r\n      localIds.push_back(this->pubId);                                          \r\n    }                                                                           \r\n                                                                                \r\n    std::copy(this->messages.begin(), this->messages.end(),                     \r\n        std::back_inserter(localBuffer));                                       \r\n    this->messages.clear();                                                     \r\n  }\r\n  ...\r\n```\r\n\r\nThis first part basically has copied everything into local buffers (presumably to avoid blocking the mutex too long and to avoid deadlocks). This is all trivial, except one crucial thing to note here: If there are any message ID's still hanging around in `this->pubID`, this function won't do anything more - no \"writing out\" happens. This can be interpreted as *\"if there are any messages which have not finished publishing yet, don't proceed, and instead keep them in the `this->messages `buffer until the next time SendMessage() is called\"*. I guess this was done in order to ensure that messages are sent out in the right order (?).\r\n\r\nNext, the copies of `messages` and `pubIDs` are used to actually publish the messages using `transport::Publication::Publish()`, which will trigger a call to `OnPublishComplete()` **once for each subscriber callback** of `Publication`. A subscriber callback however is used for **remote subscribers** only (started in a separate terminal). It's probably possible to add subscriber callbacks in different ways too. But here it's only important that if we have local subscribers (as in all the gtests as far as I can see), we won't have a subscriber callback.   \r\n*Bottom line:* `Publication::Publish()` basically returns the number of times the `OnPublishComplete()` callback has been triggered per remote subscriber (Note that with remote subscribers, the actual calling of OnPublishComplete() happens asynchronously: the subscriber callback `SubscriptionTransport::HandleData()` only enqueues the message).\r\n\r\n```C++\r\n  ...\r\n  // Only send messages if there is something to send                           \r\n  if (!localBuffer.empty())                                                     \r\n  {                                                                             \r\n    std::list<uint32_t>::iterator pubIter = localIds.begin();                   \r\n                                                                                \r\n    // Send all the current messages                                            \r\n    for (std::list<MessagePtr>::iterator iter = localBuffer.begin();            \r\n        iter != localBuffer.end(); ++iter, ++pubIter)                           \r\n    {                                                                           \r\n      // Send the latest message.                                               \r\n      int result = this->publication->Publish(*iter,                            \r\n          boost::bind(&Publisher::OnPublishComplete, this, _1), *pubIter);\r\n      ...\r\n```\r\n\r\nSo I believe the intention is to expect a certain amount of calls to `OnPublishComplete()`. To achieve this, the `this->pubIds` is set with the message ID as key, and as a value the expected number of calls:\r\n\r\n```C++\r\n      ...\r\n      if (result > 0)                                                           \r\n        this->pubIds[*pubIter] = result;                                        \r\n      else                                                                      \r\n        this->pubIds.erase(*pubIter);                                           \r\n    }\r\n    ...\r\n```\r\n\r\nPlease correct me if I have misunderstood this intention.\r\n\r\nThere is a few issues with the above bit of code:\r\n\r\n*  `pubIds` is not mutex protected as it should be.\r\n* If we have only local subscribers, the counter should still be set to 1, because `OnPublishComplete` will still be called once.\r\n* It is possible (randomly due to threads) that by the time `this->pubIds` is set to the value of `result`, `OnPublishComplete()` has already been called, and the entry in `pubIds` has already been erased. This has happened to me, which brought me onto this issue in the first place. In this case, if we have remote subscribers, we will just re-add it here, and **it will never be removed again**! Which will in turn cause all future calls to `SendMessage()` to not do anything, as per condition `!this->pubIds.empty()` in the beginning of the function. So we have a \"publishing dead-end\".\r\n\r\nThe tricky bit is that this only happens when using remote subscribers, which may be a reason why it has gone unnoticed in the automated tests so far? Because if there are only local subscribers, `result` is 0, and the buffer will just always be cleared (it will not really be used actually). So the issue won't arise at all.\r\n\r\nThis PR proposes a way to fix this up, though it's not ready to be merged as there could be better (but more intrusive) solutions to this. I've put it up for discussion. I added some more comments to the code to explain what's going on. \r\n\r\nI've also added a test file which still requires to start a remote publisher manually (by `gz topic -e <topic>`), which is something that should be simulated instead.  \r\nYou can trigger the failure case by commenting `TEMP_NEW_APPROACH_TEST` in Publisher.cc, which will then use the old code. You may have to try a few times because of the randomness, sometimes it can still succeed with the old code.\r\n\r\nOne last thing: It would be important to let the user know that if they are only using \"blocking\" calls to `Publisher::Publish()`, it is *not* guaranteed that the message has actually already been sent. It may still sit in the queue, and it may actually be necessary to call `Publisher::SendMessage()` again to make sure the queue has been emptied. See also comments in the code.", "title": "Fixing issue in which publishing of messages gets stuck.", "destination": {"commit": {"hash": "0d39ebf2b92b", "type": "commit", "links": {"self": {"href": "data/repositories/osrf/gazebo/commit/0d39ebf2b92b.json"}, "html": {"href": "#!/osrf/gazebo/commits/0d39ebf2b92b"}}}, "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "f5a8d325967a", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo/commit/f5a8d325967a"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo/commits/f5a8d325967a"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{11522dd5-648b-4e9e-b908-d3e1170ba728}ts=c_plus_plus"}}, "type": "repository", "name": "gazebo", "full_name": "JenniferBuehler/gazebo", "uuid": "{11522dd5-648b-4e9e-b908-d3e1170ba728}"}, "branch": {"name": "transport_sendmessage_blocking_issue"}}, "state": "OPEN", "author": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "date": "2017-03-27T06:06:54.392933+00:00"}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33687470.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33687470"}}, "parent": {"id": 33684600, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33684600.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33684600"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Just saw it's a timeout issue (see [jenkins output line 303](http://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/994/console)) so it's not related to the actual bug fix. Will look into it.", "markup": "markdown", "html": "<p>Just saw it's a timeout issue (see <a data-is-external-link=\"true\" href=\"http://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/994/console\" rel=\"nofollow\">jenkins output line 303</a>) so it's not related to the actual bug fix. Will look into it.</p>", "type": "rendered"}, "created_on": "2017-03-22T23:07:53.574578+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-03-22T23:08:28.848343+00:00", "type": "pullrequest_comment", "id": 33687470}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33687351.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33687351"}}, "parent": {"id": 33684600, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33684600.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33684600"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Ok\u2026 hmmm it should not fail with the macro enabled, only with it is disabled\u2026 at least it didn't fail on my machine with the fix. I'll have to look into this. I'm away for a long weekend but will try to squeeze an hour or so in for it", "markup": "markdown", "html": "<p>Ok\u2026 hmmm it should not fail with the macro enabled, only with it is disabled\u2026 at least it didn't fail on my machine with the fix. I'll have to look into this. I'm away for a long weekend but will try to squeeze an hour or so in for it</p>", "type": "rendered"}, "created_on": "2017-03-22T23:05:03.595794+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-03-22T23:05:16.335035+00:00", "type": "pullrequest_comment", "id": 33687351}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33684600.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33684600"}}, "parent": {"id": 33683777, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33683777.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33683777"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "sorry, I ran your branch on jenkins without modifications. I'm still reading the summary to get an understanding of the issue; I was just noting that the test fails in the current state of the branch", "markup": "markdown", "html": "<p>sorry, I ran your branch on jenkins without modifications. I'm still reading the summary to get an understanding of the issue; I was just noting that the test fails in the current state of the branch</p>", "type": "rendered"}, "created_on": "2017-03-22T22:01:01.628646+00:00", "user": {"display_name": "Steve Peters", "uuid": "{2ccfed09-18b8-4921-8d58-15ef01092802}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D"}, "html": {"href": "https://bitbucket.org/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/1fb4816dad9e68337d3096f750951f6cd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsSP-1.png"}}, "nickname": "Steven Peters", "type": "user", "account_id": "557058:5de38267-b118-494c-aa76-4fab35448816"}, "updated_on": "2017-03-22T22:01:01.631313+00:00", "type": "pullrequest_comment", "id": 33684600}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33683777.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33683777"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "Did you comment the `#define TEMP_NEW_APPROACH_TEST` in  Publisher.cc for the tests?", "markup": "markdown", "html": "<p>Did you comment the <code>#define TEMP_NEW_APPROACH_TEST</code> in  Publisher.cc for the tests?</p>", "type": "rendered"}, "created_on": "2017-03-22T21:43:35.049944+00:00", "user": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "updated_on": "2017-03-22T21:43:35.091120+00:00", "type": "pullrequest_comment", "id": 33683777}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33617906.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33617906"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "the [test fails](http://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/994/testReport/junit/(root)/INTEGRATION_transport_msg_count/test_ran/)", "markup": "markdown", "html": "<p>the <a data-is-external-link=\"true\" href=\"http://build.osrfoundation.org/job/gazebo-ci-pr_any-homebrew-amd64/994/testReport/junit/(root)/INTEGRATION_transport_msg_count/test_ran/\" rel=\"nofollow\">test fails</a></p>", "type": "rendered"}, "created_on": "2017-03-22T08:57:48.068124+00:00", "user": {"display_name": "Steve Peters", "uuid": "{2ccfed09-18b8-4921-8d58-15ef01092802}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D"}, "html": {"href": "https://bitbucket.org/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/1fb4816dad9e68337d3096f750951f6cd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsSP-1.png"}}, "nickname": "Steven Peters", "type": "user", "account_id": "557058:5de38267-b118-494c-aa76-4fab35448816"}, "updated_on": "2017-03-22T08:57:48.075522+00:00", "type": "pullrequest_comment", "id": 33617906}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"comment": {"links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657/comments/33617657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657/_/diff#comment-33617657"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}, "content": {"raw": "place in its alphabetical spot in the list", "markup": "markdown", "html": "<p>place in its alphabetical spot in the list</p>", "type": "rendered"}, "created_on": "2017-03-22T08:54:57.944808+00:00", "user": {"display_name": "Steve Peters", "uuid": "{2ccfed09-18b8-4921-8d58-15ef01092802}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D"}, "html": {"href": "https://bitbucket.org/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/1fb4816dad9e68337d3096f750951f6cd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsSP-1.png"}}, "nickname": "Steven Peters", "type": "user", "account_id": "557058:5de38267-b118-494c-aa76-4fab35448816"}, "inline": {}, "updated_on": "2017-03-22T08:54:57.949433+00:00", "type": "pullrequest_comment", "id": 33617657}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"update": {"description": "I believe I stumbled over a bug in the transport system which sometimes stalls the message publishing of a `transport::Publisher`. This can be triggered when using `Publisher::Publish()` in blocking mode (parameter \\_block=true).\r\n\r\nThis is a multi-threading related issue which I think should be fixed in any case (also for non-blocking mode).\r\n\r\nIt's probably easiest to explain the issue on code snippets of the existing code.\r\n\r\nFor easy reference: `OnPublishComplete` is the function which should be called at the time just after a message has just been sent (which can happen from another thread). It decreases a counter for this message ID, and deletes the entry from the buffer if the counter reaches zero. This basically helps to keep track on the number of times this function was called for each message.   \r\nNote that the `this->pubIds` map is protected by `this->mutex`.\r\n\r\n```C++\r\nvoid Publisher::OnPublishComplete(uint32_t _id) \r\n{ \r\n boost::mutex::scoped_lock lock(this->mutex); \r\n \r\n std::map<uint32_t, int>::iterator iter = this->pubIds.find(_id); \r\n if (iter != this->pubIds.end() && (--iter->second) <= 0) \r\n this->pubIds.erase(iter); \r\n} \r\n```\r\n\r\nThen there is the function `SendMessage()` which is called from `Publisher::Publish()` when in blocking mode. The blocking is supposed to wait until the message is \"written out\" (*minor side issue:* this is a phrasing in the comments that could maybe be changed, because the actual \"writing out\" still happens asynchronously from `Publication`, so a call of `Publisher::Publish(..block=true)` will not actually guarantee the message has been written out!).\r\n\r\n```C++\r\nvoid Publisher::SendMessage()                                                   \r\n{                                                                               \r\n  std::list<MessagePtr> localBuffer;                                            \r\n  std::list<uint32_t> localIds;                                                 \r\n                                                                                \r\n  {                                                                             \r\n    boost::mutex::scoped_lock lock(this->mutex);                                \r\n    if (!this->pubIds.empty() || this->messages.empty())                        \r\n      return;                                                                   \r\n                                                                                \r\n    for (unsigned int i = 0; i < this->messages.size(); ++i)                    \r\n    {                                                                           \r\n      this->pubId = (this->pubId + 1) % 10000;                                  \r\n      this->pubIds[this->pubId] = 0;                                            \r\n      localIds.push_back(this->pubId);                                          \r\n    }                                                                           \r\n                                                                                \r\n    std::copy(this->messages.begin(), this->messages.end(),                     \r\n        std::back_inserter(localBuffer));                                       \r\n    this->messages.clear();                                                     \r\n  }\r\n  ...\r\n```\r\n\r\nThis first part basically has copied everything into local buffers (presumably to avoid blocking the mutex too long and to avoid deadlocks). This is all trivial, except one crucial thing to note here: If there are any message ID's still hanging around in `this->pubID`, this function won't do anything more - no \"writing out\" happens. This can be interpreted as *\"if there are any messages which have not finished publishing yet, don't proceed, and instead keep them in the `this->messages `buffer until the next time SendMessage() is called\"*. I guess this was done in order to ensure that messages are sent out in the right order (?).\r\n\r\nNext, the copies of `messages` and `pubIDs` are used to actually publish the messages using `transport::Publication::Publish()`, which will trigger a call to `OnPublishComplete()` **once for each subscriber callback** of `Publication`. A subscriber callback however is used for **remote subscribers** only (started in a separate terminal). It's probably possible to add subscriber callbacks in different ways too. But here it's only important that if we have local subscribers (as in all the gtests as far as I can see), we won't have a subscriber callback.   \r\n*Bottom line:* `Publication::Publish()` basically returns the number of times the `OnPublishComplete()` callback has been triggered per remote subscriber (Note that with remote subscribers, the actual calling of OnPublishComplete() happens asynchronously: the subscriber callback `SubscriptionTransport::HandleData()` only enqueues the message).\r\n\r\n```C++\r\n  ...\r\n  // Only send messages if there is something to send                           \r\n  if (!localBuffer.empty())                                                     \r\n  {                                                                             \r\n    std::list<uint32_t>::iterator pubIter = localIds.begin();                   \r\n                                                                                \r\n    // Send all the current messages                                            \r\n    for (std::list<MessagePtr>::iterator iter = localBuffer.begin();            \r\n        iter != localBuffer.end(); ++iter, ++pubIter)                           \r\n    {                                                                           \r\n      // Send the latest message.                                               \r\n      int result = this->publication->Publish(*iter,                            \r\n          boost::bind(&Publisher::OnPublishComplete, this, _1), *pubIter);\r\n      ...\r\n```\r\n\r\nSo I believe the intention is to expect a certain amount of calls to `OnPublishComplete()`. To achieve this, the `this->pubIds` is set with the message ID as key, and as a value the expected number of calls:\r\n\r\n```C++\r\n      ...\r\n      if (result > 0)                                                           \r\n        this->pubIds[*pubIter] = result;                                        \r\n      else                                                                      \r\n        this->pubIds.erase(*pubIter);                                           \r\n    }\r\n    ...\r\n```\r\n\r\nPlease correct me if I have misunderstood this intention.\r\n\r\nThere is a few issues with the above bit of code:\r\n\r\n*  `pubIds` is not mutex protected as it should be.\r\n* If we have only local subscribers, the counter should still be set to 1, because `OnPublishComplete` will still be called once.\r\n* It is possible (randomly due to threads) that by the time `this->pubIds` is set to the value of `result`, `OnPublishComplete()` has already been called, and the entry in `pubIds` has already been erased. This has happened to me, which brought me onto this issue in the first place. In this case, if we have remote subscribers, we will just re-add it here, and **it will never be removed again**! Which will in turn cause all future calls to `SendMessage()` to not do anything, as per condition `!this->pubIds.empty()` in the beginning of the function. So we have a \"publishing dead-end\".\r\n\r\nThe tricky bit is that this only happens when using remote subscribers, which may be a reason why it has gone unnoticed in the automated tests so far? Because if there are only local subscribers, `result` is 0, and the buffer will just always be cleared (it will not really be used actually). So the issue won't arise at all.\r\n\r\nThis PR proposes a way to fix this up, though it's not ready to be merged as there could be better (but more intrusive) solutions to this. I've put it up for discussion. I added some more comments to the code to explain what's going on. \r\n\r\nI've also added a test file which still requires to start a remote publisher manually (by `gz topic -e <topic>`), which is something that should be simulated instead.  \r\nYou can trigger the failure case by commenting `TEMP_NEW_APPROACH_TEST` in Publisher.cc, which will then use the old code. You may have to try a few times because of the randomness, sometimes it can still succeed with the old code.\r\n\r\nOne last thing: It would be important to let the user know that if they are only using \"blocking\" calls to `Publisher::Publish()`, it is *not* guaranteed that the message has actually already been sent. It may still sit in the queue, and it may actually be necessary to call `Publisher::SendMessage()` again to make sure the queue has been emptied. See also comments in the code.", "title": "Fixing issue in which publishing of messages gets stuck.", "destination": {"commit": {"hash": "cd526975bc8b", "type": "commit", "links": {"self": {"href": "data/repositories/osrf/gazebo/commit/cd526975bc8b.json"}, "html": {"href": "#!/osrf/gazebo/commits/cd526975bc8b"}}}, "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "7bbb1a6196a0", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo/commit/7bbb1a6196a0"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo/commits/7bbb1a6196a0"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{11522dd5-648b-4e9e-b908-d3e1170ba728}ts=c_plus_plus"}}, "type": "repository", "name": "gazebo", "full_name": "JenniferBuehler/gazebo", "uuid": "{11522dd5-648b-4e9e-b908-d3e1170ba728}"}, "branch": {"name": "transport_sendmessage_blocking_issue"}}, "state": "OPEN", "author": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "date": "2017-03-15T07:56:23.630125+00:00"}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}, {"update": {"description": "I believe I stumbled over a bug in the transport system which sometimes stalls the message publishing of a `transport::Publisher`. This can be triggered when using `Publisher::Publish()` in blocking mode (parameter \\_block=true).\r\n\r\nThis is a multi-threading related issue which I think should be fixed in any case (also for non-blocking mode).\r\n\r\nIt's probably easiest to explain the issue on code snippets of the existing code.\r\n\r\nFor easy reference: `OnPublishComplete` is the function which should be called at the time just after a message has just been sent (which can happen from another thread). It decreases a counter for this message ID, and deletes the entry from the buffer if the counter reaches zero. This basically helps to keep track on the number of times this function was called for each message.   \r\nNote that the `this->pubIds` map is protected by `this->mutex`.\r\n\r\n```C++\r\nvoid Publisher::OnPublishComplete(uint32_t _id) \r\n{ \r\n boost::mutex::scoped_lock lock(this->mutex); \r\n \r\n std::map<uint32_t, int>::iterator iter = this->pubIds.find(_id); \r\n if (iter != this->pubIds.end() && (--iter->second) <= 0) \r\n this->pubIds.erase(iter); \r\n} \r\n```\r\n\r\nThen there is the function `SendMessage()` which is called from `Publisher::Publish()` when in blocking mode. The blocking is supposed to wait until the message is \"written out\" (*minor side issue:* this is a phrasing in the comments that could maybe be changed, because the actual \"writing out\" still happens asynchronously from `Publication`, so a call of `Publisher::Publish(..block=true)` will not actually guarantee the message has been written out!).\r\n\r\n```C++\r\nvoid Publisher::SendMessage()                                                   \r\n{                                                                               \r\n  std::list<MessagePtr> localBuffer;                                            \r\n  std::list<uint32_t> localIds;                                                 \r\n                                                                                \r\n  {                                                                             \r\n    boost::mutex::scoped_lock lock(this->mutex);                                \r\n    if (!this->pubIds.empty() || this->messages.empty())                        \r\n      return;                                                                   \r\n                                                                                \r\n    for (unsigned int i = 0; i < this->messages.size(); ++i)                    \r\n    {                                                                           \r\n      this->pubId = (this->pubId + 1) % 10000;                                  \r\n      this->pubIds[this->pubId] = 0;                                            \r\n      localIds.push_back(this->pubId);                                          \r\n    }                                                                           \r\n                                                                                \r\n    std::copy(this->messages.begin(), this->messages.end(),                     \r\n        std::back_inserter(localBuffer));                                       \r\n    this->messages.clear();                                                     \r\n  }\r\n  ...\r\n```\r\n\r\nThis first part basically has copied everything into local buffers (presumably to avoid blocking the mutex too long and to avoid deadlocks). This is all trivial, except one crucial thing to note here: If there are any message ID's still hanging around in `this->pubID`, this function won't do anything more - no \"writing out\" happens. This can be interpreted as *\"if there are any messages which have not finished publishing yet, don't proceed, and instead keep them in the `this->messages `buffer until the next time SendMessage() is called\"*. I guess this was done in order to ensure that messages are sent out in the right order (?).\r\n\r\nNext, the copies of `messages` and `pubIDs` are used to actually publish the messages using `transport::Publication::Publish()`, which will trigger a call to `OnPublishComplete()` **once for each subscriber callback** of `Publication`. A subscriber callback however is used for **remote subscribers** only (started in a separate terminal). It's probably possible to add subscriber callbacks in different ways too. But here it's only important that if we have local subscribers (as in all the gtests as far as I can see), we won't have a subscriber callback.   \r\n*Bottom line:* `Publication::Publish()` basically returns the number of times the `OnPublishComplete()` callback has been triggered per remote subscriber (Note that with remote subscribers, the actual calling of OnPublishComplete() happens asynchronously: the subscriber callback `SubscriptionTransport::HandleData()` only enqueues the message).\r\n\r\n```C++\r\n  ...\r\n  // Only send messages if there is something to send                           \r\n  if (!localBuffer.empty())                                                     \r\n  {                                                                             \r\n    std::list<uint32_t>::iterator pubIter = localIds.begin();                   \r\n                                                                                \r\n    // Send all the current messages                                            \r\n    for (std::list<MessagePtr>::iterator iter = localBuffer.begin();            \r\n        iter != localBuffer.end(); ++iter, ++pubIter)                           \r\n    {                                                                           \r\n      // Send the latest message.                                               \r\n      int result = this->publication->Publish(*iter,                            \r\n          boost::bind(&Publisher::OnPublishComplete, this, _1), *pubIter);\r\n      ...\r\n```\r\n\r\nSo I believe the intention is to expect a certain amount of calls to `OnPublishComplete()`. To achieve this, the `this->pubIds` is set with the message ID as key, and as a value the expected number of calls:\r\n\r\n```C++\r\n      ...\r\n      if (result > 0)                                                           \r\n        this->pubIds[*pubIter] = result;                                        \r\n      else                                                                      \r\n        this->pubIds.erase(*pubIter);                                           \r\n    }\r\n    ...\r\n```\r\n\r\nPlease correct me if I have misunderstood this intention.\r\n\r\nThere is a few issues with the above bit of code:\r\n\r\n*  `pubIds` is not mutex protected as it should be.\r\n* If we have only local subscribers, the counter should still be set to 1, because `OnPublishComplete` will still be called once.\r\n* It is possible (randomly due to threads) that by the time `this->pubIds` is set to the value of `result`, `OnPublishComplete()` has already been called, and the entry in `pubIds` has already been erased. This has happened to me, which brought me onto this issue in the first place. In this case, if we have remote subscribers, we will just re-add it here, and **it will never be removed again**! Which will in turn cause all future calls to `SendMessage()` to not do anything, as per condition `!this->pubIds.empty()` in the beginning of the function. So we have a \"publishing dead-end\".\r\n\r\nThe tricky bit is that this only happens when using remote subscribers, which may be a reason why it has gone unnoticed in the automated tests so far? Because if there are only local subscribers, `result` is 0, and the buffer will just always be cleared (it will not really be used actually). So the issue won't arise at all.\r\n\r\nThis PR proposes a way to fix this up, though it's not ready to be merged as there could be better (but more intrusive) solutions to this. I've put it up for discussion. I added some more comments to the code to explain what's going on. \r\n\r\nI've also added a test file which still requires to start a remote publisher manually (by `gz topic -e <topic>`), which is something that should be simulated instead.  \r\nYou can trigger the failure case by commenting `TEMP_NEW_APPROACH_TEST` in Publisher.cc, which will then use the old code. You may have to try a few times because of the randomness, sometimes it can still succeed with the old code.\r\n\r\nOne last thing: It would be important to let the user know that if they are only using \"blocking\" calls to `Publisher::Publish()`, it is *not* guaranteed that the message has actually already been sent. It may still sit in the queue, and it may actually be necessary to call `Publisher::SendMessage()` again to make sure the queue has been emptied. See also comments in the code.", "title": "Fixing issue in which publishing of messages gets stuck.", "destination": {"commit": {"hash": "cd526975bc8b", "type": "commit", "links": {"self": {"href": "data/repositories/osrf/gazebo/commit/cd526975bc8b.json"}, "html": {"href": "#!/osrf/gazebo/commits/cd526975bc8b"}}}, "repository": {"links": {"self": {"href": "data/repositories/osrf/gazebo.json"}, "html": {"href": "#!/osrf/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}ts=1694483"}}, "type": "repository", "name": "gazebo", "full_name": "osrf/gazebo", "uuid": "{51a0cd5d-8697-4eb1-8b08-e919ee881e1c}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "7bbb1a6196a0", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo/commit/7bbb1a6196a0"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo/commits/7bbb1a6196a0"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/JenniferBuehler/gazebo"}, "html": {"href": "https://bitbucket.org/JenniferBuehler/gazebo"}, "avatar": {"href": "data/bytebucket.org/ravatar/{11522dd5-648b-4e9e-b908-d3e1170ba728}ts=c_plus_plus"}}, "type": "repository", "name": "gazebo", "full_name": "JenniferBuehler/gazebo", "uuid": "{11522dd5-648b-4e9e-b908-d3e1170ba728}"}, "branch": {"name": "transport_sendmessage_blocking_issue"}}, "state": "OPEN", "author": {"display_name": "Jennifer Buehler", "uuid": "{5949baad-8c43-4d52-9a82-bb8c3511fed8}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D"}, "html": {"href": "https://bitbucket.org/%7B5949baad-8c43-4d52-9a82-bb8c3511fed8%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/b28ae0e95eada6ee16f0860c1fa59fdcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsJB-4.png"}}, "nickname": "JenniferBuehler", "type": "user", "account_id": "557058:96bd489a-ec14-4a06-8d31-7bb6d46d1209"}, "date": "2017-03-15T07:56:23.604576+00:00"}, "pull_request": {"type": "pullrequest", "id": 2657, "links": {"self": {"href": "data/repositories/osrf/gazebo/pullrequests/2657.json"}, "html": {"href": "#!/osrf/gazebo/pull-requests/2657"}}, "title": "Fixing issue in which publishing of messages gets stuck."}}]}